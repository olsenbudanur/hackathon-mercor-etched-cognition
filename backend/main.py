"""
FastAPI Backend for EEG-Enhanced Language Model with Mixture-of-Experts

This module implements a FastAPI backend that handles token streaming from the EEG-enhanced
language model to the frontend. It provides endpoints for adding tokens, streaming tokens,
clearing the token queue, toggling test data generation, and getting server status.

The backend serves as a bridge between the EEG processing and language model components
and the frontend visualization.
"""

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi import Body, HTTPException
import time
from typing import Dict, List, Optional
from fastapi.middleware.cors import CORSMiddleware
import random
import string
import json
import asyncio
from collections import deque
from pydantic import BaseModel

# Initialize FastAPI application
app = FastAPI(
    title="EEG-Enhanced LM Backend",
    description="Backend for EEG-Enhanced Language Model with Mixture-of-Experts",
    version="0.1.0"
)

# Configure CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Allow requests from the frontend
    allow_credentials=True,
    allow_methods=["GET", "POST"],  # Allow GET and POST methods
    allow_headers=["*"],  # Allow all headers
)

# Queue to store tokens and their associated experts
token_queue = deque(maxlen=1000)  # Limit queue size to prevent memory issues

# Configuration flags
enable_test_data = False  # Flag to control random word generation

# Expert to color mapping
EXPERT_COLORS = {
    "simple": 1,     # Maps to color 1 in frontend (e.g., blue)
    "balanced": 2,   # Maps to color 2 in frontend (e.g., green)
    "complex": 3,    # Maps to color 3 in frontend (e.g., red)
    "unknown": 2     # Default to balanced/middle color if expert is unknown
}

class TokenData(BaseModel):
    """
    Pydantic model for a single token with its associated expert.
    
    Attributes:
        token (str): The text token generated by the language model
        expert (str): The expert that generated this token (simple, balanced, or complex)
    
    Example:
        ```python
        token_data = TokenData(token="hello", expert="simple")
        ```
    """
    token: str
    expert: str

class TokenStreamData(BaseModel):
    """
    Pydantic model for a stream of tokens with their associated experts.
    
    Attributes:
        tokens (List[TokenData]): List of TokenData objects containing tokens and their experts
    
    Example:
        ```python
        stream_data = TokenStreamData(tokens=[
            TokenData(token="hello", expert="simple"),
            TokenData(token=" ", expert="balanced"),
            TokenData(token="world", expert="complex")
        ])
        ```
    """
    tokens: List[TokenData]

@app.post("/add-tokens")
async def add_tokens(data: TokenStreamData):
    """
    Endpoint to receive token data from the EEG demo and add it to the streaming queue.
    
    This endpoint receives tokens generated by the language model along with their
    associated experts, processes them, and adds them to the token queue for streaming
    to the frontend.
    
    Args:
        data (TokenStreamData): A TokenStreamData object containing a list of tokens
                               and their associated experts
    
    Returns:
        dict: A dictionary with status information and the number of tokens added
              - status (str): "success" if tokens were added successfully
              - tokens_added (int): The number of tokens that were added to the queue
    
    Example:
        ```python
        import requests
        import json
        
        response = requests.post(
            "http://localhost:8000/add-tokens",
            json={
                "tokens": [
                    {"token": "Hello", "expert": "simple"},
                    {"token": " ", "expert": "balanced"},
                    {"token": "world", "expert": "complex"}
                ]
            }
        )
        print(response.json())  # {"status": "success", "tokens_added": 3}
        ```
    """
    for token_data in data.tokens:
        # Add each token with its expert to the queue
        token = token_data.token
        expert = token_data.expert
        
        # Validate and process token text
        # Ensure token is a non-empty string
        if not token and token != " ":  # Special check for space tokens
            continue
        
        # Ensure token is properly escaped for display
        token = token.replace('\\', '\\\\').replace('\n', ' ').replace('\t', ' ')
        
        # Limit token length for display (frontend might have size constraints)
        if len(token) > 30:
            token = token[:27] + '...'
        
        # Map expert to a color number for the frontend
        color_num = EXPERT_COLORS.get(expert.lower(), 2)
        
        token_queue.append({
            "word": token,
            "number": color_num
        })
    
    return {"status": "success", "tokens_added": len(data.tokens)}

# Fallback function for demo/testing - generates random words
def generate_random_token():
    """
    Generate a random token with an associated expert for testing purposes.
    
    This function creates realistic token samples for testing the frontend visualization
    when no real EEG data is available. It generates different types of tokens (words,
    punctuation, spaces, abbreviations, numbers) with appropriate expert assignments
    based on token characteristics.
    
    Returns:
        dict: A dictionary containing the generated token and its associated expert color
              - word (str): The generated token text
              - number (int): The color number corresponding to the expert (1-3)
    
    Example:
        ```python
        token = generate_random_token()
        print(token)  # {'word': 'the', 'number': 1}
        ```
    """
    # Possible token types
    token_types = [
        "word",          # Normal word
        "punctuation",   # Punctuation mark
        "space",         # Space or whitespace
        "abbreviation",  # Abbreviation or acronym
        "number",        # Numeric token
    ]
    
    # Weight toward normal words
    token_type = random.choices(
        token_types, 
        weights=[0.7, 0.15, 0.05, 0.05, 0.05], 
        k=1
    )[0]
    
    if token_type == "word":
        # Common English words
        common_words = [
            "the", "and", "that", "have", "for", "not", "with", "you", 
            "this", "but", "from", "they", "say", "she", "will", "one", 
            "all", "would", "there", "their", "what", "out", "about", 
            "who", "get", "which", "when", "make", "can", "like", "time", 
            "just", "know", "people", "year", "take", "them", "some", 
            "into", "two", "see", "more", "look", "only", "come", "its", 
            "over", "think", "also", "back", "use", "after", "work", 
            "first", "well", "way", "even", "new", "want", "because", 
            "any", "these", "give", "day", "most", "us"
        ]
        random_word = random.choice(common_words)
        
        # Occasionally make it uppercase or capitalize
        if random.random() < 0.05:  # 5% chance
            random_word = random_word.upper()
        elif random.random() < 0.2:  # 20% chance
            random_word = random_word.capitalize()
            
    elif token_type == "punctuation":
        # Common punctuation
        random_word = random.choice([",", ".", "!", "?", ";", ":", "-", ")", "(", "\"", "'"])
        
    elif token_type == "space":
        # Space or newline
        random_word = " "
        
    elif token_type == "abbreviation":
        # Common abbreviations or acronyms
        random_word = random.choice(["AI", "ML", "Dr.", "Mr.", "PhD", "USA", "UK", "CEO", "etc.", "e.g."])
        
    else:  # number
        # Numeric token
        if random.random() < 0.7:  # 70% chance of a single digit
            random_word = str(random.randint(0, 9))
        else:  # 30% chance of a multi-digit number
            random_word = str(random.randint(10, 999))
    
    # Assign a realistic expert based on the token type
    if token_type == "word" and len(random_word) >= 7:
        # Longer words tend to use the complex expert
        expert_weights = {"simple": 0.1, "balanced": 0.3, "complex": 0.6}
    elif token_type in ["punctuation", "space"]:
        # Punctuation and spaces tend to use the simple expert
        expert_weights = {"simple": 0.7, "balanced": 0.25, "complex": 0.05}
    elif token_type == "abbreviation":
        # Abbreviations tend to use the complex expert
        expert_weights = {"simple": 0.2, "balanced": 0.3, "complex": 0.5}
    else:
        # Most tokens tend to use the balanced expert
        expert_weights = {"simple": 0.3, "balanced": 0.5, "complex": 0.2}
    
    # Select expert based on weights
    experts = list(expert_weights.keys())
    weights = list(expert_weights.values())
    random_expert = random.choices(experts, weights=weights, k=1)[0]
    
    return {
        "word": random_word,
        "number": EXPERT_COLORS[random_expert]
    }

# Streaming function to yield tokens from the queue
async def event_stream():
    """
    Stream tokens to the frontend in a controlled manner using server-sent events.
    
    This asynchronous generator function continuously streams tokens from the token queue
    to the frontend. If the queue is empty and test data generation is enabled, it will
    generate random test tokens. The function includes realistic delays between tokens
    to simulate the natural rhythm of text generation.
    
    Yields:
        str: Server-sent event formatted string containing token data in JSON format
    
    Example:
        This function is typically used with FastAPI's StreamingResponse:
        ```python
        @app.get("/stream")
        async def stream():
            return StreamingResponse(event_stream(), media_type="text/event-stream")
        ```
    """
    while True:
        if token_queue:
            # If there are tokens in the queue, send one
            data = token_queue.popleft()
            
            # Debug info
            word = data.get("word", "[empty]")
            number = data.get("number", 0)
            print(f"Streaming token: '{word}' (Expert color: {number})")
            
            # Send as server-sent event
            yield f"data: {json.dumps(data)}\n\n"
            
            # Realistic delay between tokens (varies slightly)
            # This simulates the natural rhythm of text generation
            delay = 0.1 + random.random() * 0.1  # 0.1-0.2 seconds
            await asyncio.sleep(delay)  
        else:
            # If queue is empty and test data is enabled, generate random tokens
            # Otherwise, just wait for new tokens
            if enable_test_data:
                data = generate_random_token()
                # Debug info
                word = data.get("word", "[empty]")
                number = data.get("number", 0)
                print(f"Streaming test token: '{word}' (Expert color: {number})")
                
                # Send as server-sent event
                yield f"data: {json.dumps(data)}\n\n"
                
                # Longer delay for test mode (feels more natural)
                await asyncio.sleep(1.0 + random.random() * 1.0)  # 1-2 seconds
            else:
                # Just wait for new tokens without sending anything
                await asyncio.sleep(0.2)

@app.get("/stream")
async def stream():
    """
    Stream tokens to the frontend using server-sent events.
    
    This endpoint establishes a persistent connection with the frontend and
    streams tokens one by one as they become available in the token queue.
    
    Returns:
        StreamingResponse: A streaming response that yields tokens as server-sent events
    
    Example:
        This endpoint is typically consumed by an EventSource in JavaScript:
        ```javascript
        const eventSource = new EventSource('/stream');
        eventSource.onmessage = (event) => {
            const data = JSON.parse(event.data);
            console.log('Received token:', data.word, 'Expert:', data.number);
        };
        ```
    """
    return StreamingResponse(event_stream(), media_type="text/event-stream")

# Endpoint to clear the token queue (useful for testing)
@app.post("/clear-tokens")
async def clear_tokens():
    """
    Clear all tokens from the token queue.
    
    This endpoint is useful for testing and resetting the state of the application.
    
    Returns:
        dict: A dictionary with status information
              - status (str): "success" if the queue was cleared successfully
              - message (str): A message indicating that the token queue was cleared
    
    Example:
        ```python
        import requests
        
        response = requests.post("http://localhost:8000/clear-tokens")
        print(response.json())  # {"status": "success", "message": "Token queue cleared"}
        ```
    """
    token_queue.clear()
    return {"status": "success", "message": "Token queue cleared"}

# Endpoint to toggle test data generation
@app.post("/toggle-test-data")
async def toggle_test_data(enable: bool = Body(..., embed=True)):
    """
    Enable or disable random test data generation.
    
    When test data generation is enabled, the server will generate random tokens
    when the token queue is empty, which is useful for testing the frontend
    without real EEG data.
    
    Args:
        enable (bool): Whether to enable (True) or disable (False) test data generation
    
    Returns:
        dict: A dictionary with status information
              - status (str): "success" if the setting was updated successfully
              - test_data_enabled (bool): The current state of test data generation
    
    Example:
        ```python
        import requests
        
        response = requests.post(
            "http://localhost:8000/toggle-test-data",
            json={"enable": True}
        )
        print(response.json())  # {"status": "success", "test_data_enabled": true}
        ```
    """
    global enable_test_data
    enable_test_data = enable
    return {"status": "success", "test_data_enabled": enable_test_data}

# Get current status
@app.get("/status")
async def get_status():
    """
    Get the current status of the server.
    
    This endpoint returns information about the current state of the server,
    including the number of tokens in the queue and whether test data generation
    is enabled.
    
    Returns:
        dict: A dictionary with status information
              - queue_size (int): The number of tokens currently in the queue
              - test_data_enabled (bool): Whether test data generation is enabled
    
    Example:
        ```python
        import requests
        
        response = requests.get("http://localhost:8000/status")
        print(response.json())  # {"queue_size": 10, "test_data_enabled": false}
        ```
    """
    return {
        "queue_size": len(token_queue),
        "test_data_enabled": enable_test_data
    }

# Get recent tokens for debugging
@app.get("/debug-tokens")
async def debug_tokens(limit: int = 50):
    """
    Get a dump of recently processed tokens for debugging purposes.
    
    This endpoint returns a list of tokens currently in the queue, which is useful
    for debugging and monitoring the state of the application.
    
    Args:
        limit (int, optional): Maximum number of tokens to return. Defaults to 50.
    
    Returns:
        dict: A dictionary with debug information
              - queue_size (int): The number of tokens currently in the queue
              - tokens (list): A list of token dictionaries, limited by the limit parameter
              - test_data_enabled (bool): Whether test data generation is enabled
    
    Example:
        ```python
        import requests
        
        response = requests.get("http://localhost:8000/debug-tokens?limit=10")
        print(response.json())
        # {
        #   "queue_size": 15,
        #   "tokens": [{"word": "Hello", "number": 1}, ...],
        #   "test_data_enabled": false
        # }
        ```
    """
    # Create a copy of the token queue for inspection
    token_list = list(token_queue)
    
    return {
        "queue_size": len(token_queue),
        "tokens": token_list[:limit],
        "test_data_enabled": enable_test_data
    }

# Set debug mode on startup (but not test data by default)
@app.on_event("startup")
async def startup_event():
    """
    Initialize the application on startup.
    
    This function is called when the FastAPI application starts up. It sets
    the debug mode and initializes the test data generation flag.
    """
    app.debug = True  # Set to False in production
    global enable_test_data
    enable_test_data = False

def generate_test_tokens():
    """
    Generate random test tokens for development and testing.
    
    This function creates a list of random tokens with associated experts for
    testing the token streaming functionality without real EEG data.
    
    Returns:
        list: A list of TokenData objects containing random tokens and experts
    
    Example:
        ```python
        tokens = generate_test_tokens()
        for token in tokens:
            print(f"Token: {token.token}, Expert: {token.expert}")
        ```
    """
    words = ["The", "quick", "brown", "fox", "jumps", "over", "lazy", "dog", 
             "Hello", "world", "This", "is", "a", "test", "of", "token", "streaming",
             "with", "different", "experts", "handling", "various", "parts", 
             "of", "the", "sentence", ".", "!", "?", ",", "and", "more", " "]
             
    experts = ["simple", "balanced", "complex"]
    
    # Generate 1-3 tokens
    num_tokens = random.randint(1, 3)
    tokens = []
    
    for _ in range(num_tokens):
        # Ensure we have some spaces in the stream
        if random.random() < 0.2:
            text = " "  # Use a space token 20% of the time
        else:
            text = random.choice(words)
            
        expert = random.choice(experts)
        tokens.append(TokenData(token=text, expert=expert))
        
    return tokens
